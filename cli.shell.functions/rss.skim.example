#!/bin/bash

##
## shell-hack: rss reader functions 
##             a usage example for combining the gr_* line-based text filters
##

#   (actually a quite useful hack for me to cope with a few huge threads,
#    which would otherwise be absolutely overwhelming when e.g accessed
#    with newsfox or newsbeuter - it looks like rss readers are still
#    immature and lack proper news reader filtering capabilities. Then
#    again I've never found _sufficient_ filtering/killfile support
#    outside of the ancient nn news reader... . And GUI apps as a rule
#    seem to be the least powerful of news readers - probably part of
#    the reason for the long&slow decline of Usenet).


# last change: 201002
# copyright:  (c) 2010 jakobi@acm.org, GPL v3 or later
# archive:    http://jakobi.github.com/script-archive-doc/

# usage:
# - invoke to demonstrate example for manga rss feeds 
#   (option -n to skip some steps for testing)
# - source and use rss_skim function 

. ~/bin/shell/grep.func


# example rss "reader" as an usage example for gr_NEW
# (gr_NEW is basically a variant of "uniq": a line-based filter
#  with memory file to suppress duplicates and previously seen lines.
#
# the rss "reader" has a memory cache in $base, with additional manual
# kill and select file containing regexes. interactive choice of descriptions
# and links within an editor, link-extraction, and finally forwarding the links
# to a remote firefox instance.
#
# the reader skips previously seen items as well as previously seen "threads", 
# unless the thread is expressly matched by a user entered regex in the select file:
#
# dupe detection can be performed on a subset of the lines passing thru
# gr_NEW by using the -p option to insert of \x01 marks, which
# advise gr_NEW to ignore anything inbetween. A subsequent gr_STRIPX1
# can remove the \x01 afterwards.

# dependencies and possible replacements:
# - rsstail/cat0par (could be substituted by wget (or lynx)
#   and cat0par, or done entirely in perl; cat0par just
#   massages rsstail or xml <item>...</item> into grep-able
#   single line items with a fixed format cum link).
# - Grep.pm (can be replaced by GNU egrep, with some
#   restrictions on the regexes)
# - grep.func functions gr_NEW,gr_UNIQ (just copy the shell
#   functions and perl one-liners into .bashrc or similar)
# - pipe.vim (basically any editor suitable for use _within_
#   a pipe)
# - firefoxstdin/firefox wrappers (a perl -lpe loop invoking
#   firefox-3.5 to remotely load urls into tabs would suffice)


# debugging helpers
function SKP  { cat; }
function DERR { tee /dev/stderr; }

function rss_skip_date_link {
   # skip Link and Pub.date (helps if the feed likes to stupidly change Pub.date
   # for already published items)
   # works if Title: is the before either in the rss item (as formatted by rsstail/cat0par)
   perl -lpe 's/(?<=[^\x01])(Pub.date:|Link:)/\x01$&/'
}

function rss_skim {
   typeset auto base opt1 mangle per_feed
   opt1="-#"
   auto=""
   mangle=""

   # set to SKP for a combine core cache and strip the feedurl prefix in the cache
   per_feed="" 

   base=$1; shift
   [ "$base" = "" ] && { echo "! no filename given as arg 1" 1>&2; return 20; }
   while [ $# -gt 0 ]; do
      case "$1" in
         -n) auto=SKP; opt1="-n" ;;
         -p) mangle="-p $2"; shift    ;;
         *)  break               ;;
      esac
      shift
   done

   echo "# using cache $base*"
   # echo "# RSS on `date`" >> $base
   echo "# RSS on `date`" >> $base.chosen # just log the results from vim.pipe, not used otherwise

   # Description: 
   # The core is merely this straight-forward pipe of basic line-based text filters:
   #
   # <get_feed> | <grep kill> | <gr_NEW skip dupes*> | <gr_NEW skip dupe threads unless selected*> | <grep highlight selected> | <editor> | <firefox>
   #                                              *) stages update cache files for future pipe invocations
   #
   # get the feeds (rsstail could be replaced with 
   # wget/cat0par -nl -nonl -start '<item>' -end '</item>'),
   # skip the last 500 seen entries (in $base.feed, 1st gr_NEW), 
   # apply killfile, remove dupes with the main cache ($base,
   # 2nd gr_NEW, skipping $base.select cache entries, but log
   # surviving items in $base.new, possibly mangling (gr_NEW -p) the lines
   # to just a string of interest by skipping anything between
   # \x01 during dupe detection wrt the main cache), 
   # sort, highlight matches from .select, strip \x01,
   # show the list to the user in an editor, strip 
   # the highlighting and just forward the link.
   # run the links thru firefoxstdin.
   #
   # Optional final step: update $base with $base.new to remember
   # the new threads/items in the 2nd gr_NEW, either manually
   # or automatically (gr_NEWUPD call below).
   
   # Notes:
   # - the dupe detection by gr_UNIQ is just a variant of the old trick
   #   "cat <SEEN_ENTRIES> <SEEN_ENTRIES> - | uniq" with a slightly modified uniq.
   # - the grep-based select/kill sees any \x01 as ordinary character,
   #   while GR_NEW will skip substrings when called with -p and a
   #   text mangling function/command (gr_NEW just wraps the cache
   #   management around the dupe-detecting gr_UNIQ -U). This is the
   #   trick used for 'threading' items.
   # - set PERL_UNICODE=63 for unicode (the gr_* are mostly
   #   perl oneliners for ease of use)
   # - set gr_UNIQ_IGNCASE to ignore case in dupe detection
   # - dupe detecting gr_NEW would profit from a pure-perl
   #   rewrite. It also lacks support for \0-lines.
   #   (maybe use catv as a base?)
   # - restricted core cache to detect dupes only within a single feed:
   #   - remove the corresponding two perl oneliners to do global thread dupe detection
   #     (or set/unset the per_feed variable above)
   #   - if the feed url changes - to avoid amnesia -  you need to update the cache, 
   #     as the feed url is the first word of each cached line.
 
   # Sketch to modify this pipe to generate rss streams:
   # - First remove the final firefoxstdin, and possibly also remove pipe.vim
   #   (otherwise you might wish to set EDITOR to gvim or another graphical
   #   editor opening its own window). Change rsstail to wget and cat0par to
   #   cat0par -nl -nonl -start '<item>' -end '</item>'. Maybe retain some header
   #   tags as well. 
   # - If the tag ordering within items is not stable, use a line of perl
   #   to force a fixed ordering of sub-elements (CPAN probably should 
   #   offer more complete modules for string mangling; which could substitute
   #   for both ordering hack and cat0par and probably also our use of wget.
   

   for i in "$@"  ; do 
      echo "" 1>&2 
      echo "# feed $i" 1>&2 
      rsstail -1 -v -p -l -u "$i" |\
      cat0par -nl -nonl -start 'Title:.*' -end 'Pub.date:.*' |\
      grep "^Title" |\
      Grep.pm -iv -f $base.kill - |\
      gr_NEW -p rss_skip_date_link $opt1                 $base.feed |\
      gr_STRIPX1 |\
      i=$i $per_feed perl -lpe '$_="$ENV{i} $_"   # allow dupe detection only within a single feed' |\
      gr_NEW $mangle                     -k $base.select $base |\
      gr_STRIPX1 |\
      i=$i $per_feed perl -lpe 's/^\Q$ENV{i} \E// # allow dupe detection only within a single feed' |\
      sort -f
   done |\
   Grep.pm -AiU -f $base.select - |\
   $auto pipe.exec pipe.vim -header "# remaining rss items will be fed to firefox for reading." |\
   perl -lpe 's/.*Link: (\S+).*/$1/; s/>>>_|_<<<//g; s!(onemanga.com/[^/]+)/[\d\.]+!$1!' |\
   tee -a $base.chosen |\
   $auto firefoxstdin -3
      
   # the feed cache just serves to make the currently fetched feed uniq
   # truncate some files
   if [ "$opt1" = "-#" ]; then
      # gr_NEWUPD -h        $base         # latest _first_; update core cache 
      #                                   # (this does "threading" as part of the dupe detection)
      #                                   # (currently done for the main cache when this file
      #                                   #  is executed as a script, see the IF-caller stanza
      #                                   #  below).
      #                                   # ! DO NOT TRIM THIS FILE ! (except maybe semi-manual
      #                                   # removing of entries last updated a few years ago...,
      #                                   # and regular comment elimination)
      gr_TRIM     -t -2000  $base.feed    # latest last;    currently do update and trim 
      gr_TRIM     -t -500   $base.chosen  # latest last;    the other caches immediately
   fi

   echo ""
   echo "# filtering procedure: first the rss feed is filtered against the"
   echo "# .kill file and the .feed cache containing the 500 most recent rss items."
   echo "# the title prefix is then matched against the core cache. if however the"
   echo "# item is matched by .select, the core cache deduplication is skipped."
   echo "# core cache maintenance is mostly automatic, .kill/.select is manual."
   echo "#"
#   echo "# to update the core cache, run (avoid editing within lines)"
   echo "#      ( b=$base; gr_NEWUPD -t \"\$b\" )"
#   echo "#   or (to automatically trim lines for the same story with older timestamps)" 
#   echo "#      function M { perl -lpe 's/(?<=[^\x01])(, +(Vol\.|Ch\.)| +Chapter )/\x01$&/'; }"
#   echo "#      b=$base; gr_NEWUPD -t -p M \"\$b\""
#   echo "#      (also considered dupes: multiple feeds with the exact same title (incl. case))"
#   echo "# to update the Grep.pm selection patterns for already seen manga"
#   echo "#      b=$base; cat \"\$b\".select.new >> \"\$b\".select"
   echo "#      b=$base; vi \"\$b\".select # or .kill; edit regexes for select/kill"
#   echo "#      (.select is raw and may require regexification!)"
   ls -s $base $base.*
}



##
## example setup for some manga feeds
##

# substring of interest for cache-based dupe detection (instead of whole line)
function mangamangle { 
   perl -lpe 's/(?<=[^\x01])(, +(Vol\.|Ch\.)| +Chapter )/\x01$&/'
   # caveat:
   # - the cache currently suppresses ANY
   #   new chapter for a series already in the cache.
   #   only new entries and .select survive.
}

# run automatically UNLESS the file is sourced (sourcing would allow you to
# call the rss_skim function with other feeds and possibly different mangling/
# core cache update behaviour.
if [ "`caller`" = "0 NULL" ]; then
   rss_skim ~/.manga -p mangamangle ${1:+"$@"} http://feeds.feedburner.com/mangafox/latest_manga_chapters?format=xml http://www.onemanga.com/updates-feed.xml

   # immediately update the cache
   function M { perl -lpe 's/(?<=[^\x01])(, +(Vol\.|Ch\.)| +Chapter )/\x01$&/'; }
   grep -v '^#' ~/.manga.new >/dev/null && gr_NEWUPD -h -p M ~/.manga 2>&1 | fgrep -v '# gr_NEWUPD: ( echo "# gr_NEWUPD:'
fi

