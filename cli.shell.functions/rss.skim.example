#!/bin/bash

##
## shell-hack: rss reader functions
##             a usage example for combining the gr_* line-based text filters
##

#   (actually a quite useful hack for me to cope with a few huge threads,
#    which would otherwise be absolutely overwhelming when e.g accessed
#    with newsfox or newsbeuter - it looks like rss readers are still
#    immature and lack proper news reader filtering capabilities. Then
#    again I've never found _sufficient_ filtering/killfile support
#    outside of the ancient nn news reader... . And GUI apps as a rule
#    seem to be the least powerful of news readers - probably part of
#    the reason for the long&slow decline of Usenet).


# last change: 201002
# copyright:  (c) 2010 jakobi@acm.org, GPL v3 or later
# archive:    http://jakobi.github.com/script-archive-doc/
# status: hack/example

# usage:
# - invoke to demonstrate example for manga rss feeds 
#   (option -n to skip some steps for testing)
# - source and use rss_skim function 

. ~/bin/shell/grep.func


# example rss "reader" as an usage example for gr_NEW
# (gr_NEW is basically a variant of "uniq": a line-based filter
#  with memory file to suppress duplicates and previously seen lines.
#
# the rss "reader" has a memory cache in $base, with additional manual
# kill and select file containing regexes. interactive choice of descriptions
# and links within an editor, link-extraction, and finally forwarding the links
# to a remote firefox instance.
#
# the reader skips previously seen items as well as previously seen "threads", 
# unless the thread is expressly matched by a user entered regex in the select file:
#
# dupe detection can be performed on a subset of the lines passing thru
# gr_NEW by using the -p option to insert of \x01 marks, which
# advise gr_NEW to ignore anything inbetween. A subsequent gr_STRIPX1
# can remove the \x01 afterwards.

# dependencies and possible replacements:
# - rsstail/cat0par (could be substituted by wget (or lynx)
#   and cat0par, or done entirely in perl; cat0par just
#   massages rsstail or xml <item>...</item> into grep-able
#   single line items with a fixed format plus link). See rsstail1 
#   hack below
# - Grep.pm (can be replaced by GNU egrep, with some
#   restrictions on the regexes)
# - grep.func functions gr_NEW,gr_UNIQ (just copy the shell
#   functions and perl one-liners into .bashrc or similar)
# - pipe.vim (basically any editor suitable for use _within_
#   a pipe)
# - firefoxstdin/firefox wrappers (a perl -lpe loop invoking
#   firefox-3.5 to remotely load urls into tabs would suffice)


# debugging helpers
function SKP  { cat; }
function DERR { tee /dev/stderr; }

rsstail=rsstail1 # real rsstail binary or hack below
function rsstail1 {
   # emulate rsstail -1 ... -u URL
   typeset f
   eval 'f=$'$#
   wget -O - $f 2>/dev/null |\
   cat0par -nl -nonl -f inside -start '(?i)<item.*?>' -end '(?i)</item.*?>' 2>/dev/null |\
   perl -lne 'sub t { return "$_[0]: $1 " if m!<$_[1]\b.*?>\s*(.*?)\s*</$_[1]\b.*?>!i; return ""};
      $o=""; $o.=t("Title","title"); 
      $l="";
      $l=t("Link","feedburner:origlink") if not $l;
      $l=t("Link","link") if not $l; 
      $o.=$l; 
      $o.=t("Pub.date","pubdate"); 
      print $o if $o'
}

function rss_skip_date_link {
   # skip Link and Pub.date (helps if the feed likes to stupidly change Pub.date
   # for already published items)
   # works if Title: is the before either in the rss item (as formatted by rsstail/cat0par)
   perl -lpe 's/(?<=[^\x01])(Pub.date:|Link:)/\x01$&/'
}

function rss_skim {
   typeset auto base opt1 mangle per_feed numitem augment
   numitem=1000
   opt1="-#"
   auto=""
   mangle=""
   augment=cat

   # set to SKP for a combine core cache and strip the feedurl prefix in the cache
   per_feed="" 

   base=$1; shift
   [ "$base" = "" ] && { echo "! no filename given as arg 1" 1>&2; return 20; }
   while [ $# -gt 0 ]; do
      case "$1" in
         -n) auto=SKP; opt1="-n" ;;
         -p) mangle="-p $2"; shift    ;;
         -P) augment="$2";   shift    ;;
         *)  break               ;;
      esac
      shift
   done

   echo "# using cache $base*"
   test -f "$base"        || touch "$base"
   test -f "$base.select" || touch "$base.select"
   test -f "$base.kill"   || touch "$base.kill"
   test -f "$base.feed"   || touch "$base.feed"

   # echo "# RSS on `date`" >> $base
   # echo "# RSS on `date`" >> $base.chosen # just log the results from vim.pipe, not used otherwise

   # Description: 
   # The core is merely this straight-forward pipe of basic line-based text filters:
   #
   # <get_feed> | <grep kill> | <gr_NEW skip dupes*> | <gr_NEW skip dupe threads unless selected*> | <grep highlight selected> | <editor> | <firefox>
   #                                              *) stages update cache files for future pipe invocations
   #
   # get the feeds using rsstail or the above rsstail1 hack
   # skip the last 500 seen entries (in $base.feed, 1st gr_NEW), 
   # apply killfile, remove dupes with the main cache ($base,
   # 2nd gr_NEW, skipping $base.select cache entries, but log
   # surviving items in $base.new, possibly mangling (gr_NEW -p) the lines
   # to just a string of interest by skipping anything between
   # \x01 during dupe detection wrt the main cache), 
   # sort, highlight matches from .select, strip \x01,
   # show the list to the user in an editor, strip 
   # the highlighting and just forward the link.
   # run the links thru firefoxstdin.
   #
   # Optional final step: update $base with $base.new to remember
   # the new threads/items in the 2nd gr_NEW, either manually
   # or automatically (gr_NEWUPD call below).
   
   # Notes:
   # - the dupe detection by gr_UNIQ is just a variant of the old trick
   #   "cat <SEEN_ENTRIES> <SEEN_ENTRIES> - | uniq" with a slightly modified uniq.
   # - the grep-based select/kill sees any \x01 as ordinary character,
   #   while GR_NEW will skip substrings when called with -p and a
   #   text mangling function/command (gr_NEW just wraps the cache
   #   management around the dupe-detecting gr_UNIQ -U). This is the
   #   trick used for 'threading' items.
   # - set PERL_UNICODE=63 for unicode (the gr_* are mostly
   #   perl oneliners for ease of use)
   # - set gr_UNIQ_IGNCASE to ignore case in dupe detection
   # - dupe detecting gr_NEW would profit from a pure-perl
   #   rewrite. It also lacks support for \0-lines.
   #   (maybe use catv as a base?)
   # - restricted core cache to detect dupes only within a single feed:
   #   - remove the corresponding two perl oneliners to do global thread dupe detection
   #     (or set/unset the per_feed variable above)
   #   - if the feed url changes - to avoid amnesia -  you need to update the cache, 
   #     as the feed url is the first word of each cached line.
 
   # Sketch to modify this pipe to generate rss streams:
   # - First remove the final firefoxstdin, and possibly also remove pipe.vim
   #   (otherwise you might wish to set EDITOR to gvim or another graphical
   #   editor opening its own window). Change rsstail to wget and cat0par to
   #   cat0par -nl -nonl -start '<item>' -end '</item>'. Maybe retain some header
   #   tags as well. 
   # - If the tag ordering within items is not stable, use a line of perl
   #   to force a fixed ordering of sub-elements (CPAN probably should 
   #   offer more complete modules for string mangling; which could substitute
   #   for both ordering hack and cat0par and probably also our use of wget).
   

   for i in "$@"  ; do 
      echo "" 1>&2 
      echo "# feed $i" 1>&2 
      $rsstail -n $numitem -1 -v -p -l -u "$i" |\
      cat0par -nl -nonl -start 'Title:.*' -end 'Pub.date:.*' |\
      grep "^Title" |\
      perl -lne 'print; s/LINK: \S*//i; $o=$_ if /\S/ and not $o; END{print main::STDERR "#last line: $o"};' |\
      Grep.pm -iv -f $base.kill - |\
      gr_NEW -p rss_skip_date_link $opt1                 $base.feed |\
      gr_STRIPX1 |\
      i=$i $per_feed perl -lpe '$_="$ENV{i} $_";"allow dupe detection only within a single feed"' |\
      gr_NEW $mangle                     -k $base.select $base      |\
      gr_STRIPX1 |\
      i=$i $per_feed perl -lpe 's/^\Q$ENV{i} \E// # allow dupe detection only within a single feed' |\
      sort -f
   done |\
   $augment |\
   Grep.pm -AiU -f $base.select - |\
   tee -a /dev/stderr |\
   $auto pipe.exec pipe.vim -header "# `date +"%Y%m%d-%H%M:"` remaining rss items will be fed to firefox." |\
#   pipe.exec pipe.vim -header "# `date +"%Y%m%d-%H%M:"` remaining rss items will be fed to firefox." |\
   pipe.exec -i grep '.' |\
   tee    $base.chosen.new |\
   tee -a $base.chosen |\
   perl -lpe 's/.*Link: (\S+).*/$1/; s/>>>_|_<<<//g; s!(onemanga.com/[^/]+)/[\d\.]+!$1!' |\
   tee -a $base.chosen-links |\
cat ###  
###$auto firefoxstdin -3
      
   # the feed cache just serves to make the currently fetched feed uniq
   # truncate some files
   if [ "$opt1" = "-#" ]; then
      # gr_NEWUPD -h        $base         # latest _first_; update core cache 
      #                                   # (this does "threading" as part of the dupe detection)
      #                                   # (currently done for the main cache when this file
      #                                   #  is executed as a script, see the IF-caller stanza
      #                                   #  below).
      #                                   # ! DO NOT TRIM THIS FILE ! (except maybe semi-manual
      #                                   # removing of entries last updated a few years ago...,
      #                                   # and regular comment elimination)
      gr_TRIM     -t -2000  $base.feed    # latest last;    currently do update and trim 
      gr_TRIM     -t -500   $base.chosen  # latest last;    the other caches immediately
      gr_TRIM     -t -500   $base.chosen-links 
   fi

   echo ""
   echo "# filtering procedure: first the rss feed is filtered against the"
   echo "# .kill file and the .feed cache containing the 500 most recent rss items."
   echo "# the (feed url+) title prefix is then matched against the core cache."
   echo "# if however the item is matched by .select, the core cache item/thread"
   echo "# deduplication is skipped."
   echo "# core cache maintenance is automatic, regexes in .kill/.select are manual."
   echo "#"
#   echo "# to update the core cache, run (avoid editing within lines)"
   echo "#      ( b=$base; gr_NEWUPD -t \"\$b\" )"
#   echo "#   or (to automatically trim lines for the same story with older timestamps)" 
#   echo "#      function M { perl -lpe 's/(?<=[^\x01])(, +(Vol\.|Ch\.)| +Chapter )/\x01$&/'; }"
#   echo "#      b=$base; gr_NEWUPD -t -p M \"\$b\""
#   echo "#      (also considered dupes: multiple feeds with the exact same title (incl. case))"
#   echo "# to update the Grep.pm selection patterns for already seen manga"
#   echo "#      b=$base; cat \"\$b\".select.new >> \"\$b\".select"
   echo "#      b=$base; vi \"\$b\".select # or .kill; edit regexes manually"
#   echo "#      (.select is raw and may require regexification!)"
   ls -s $base $base.*
}



##
## example setup of the reader for some manga feeds
##



# substring of interest for cache-based dupe detection (instead of whole line)
function mangamangle { 
   perl -lpe 's/(?<=[^\x01])(, +(Vol\.|Ch\.)| +Chapter )/\x01$&/'
   # caveat:
   # - the cache currently suppresses ANY
   #   new chapter for a series already in the cache.
   #   only new entries and .select survive.
}

# try augmenting the line presented to the user with
# chapter/date information from the cache (the previous chapter/datestamp)
function mangaaugment {
   perl -lpe 'BEGIN{
                 $f=shift;
                 open(FH,"<","$f");
                 while(<FH>){
                    $d=$c=$t="";
                    s/\S+ //; # strip feed name
                    /date:\s+.*?(\d+ [a-z]{3} \d+)/i and $d=$1;
                    s/ (Link|pub.?date):.*//i; 
                    /(, +(Vol\.|Ch\.)| +Chapter ).*/ and ($t,$c)=($`,$&); 
                    $c=~s/^[, ]+//g; 
                    $c=~s/Vol\.?|Chapter |Ch\.//g; 
                    $c{$t}=$c;
                    $c{$t}.="; ".$d if $d;
                 }
              }; 
              s/(, +(Vol\.|Ch\.)| +Chapter ).*?(?=Link:|pub.?date)/$& [$c{$`}] /i;
        ' ~/.manga
}

# usage: $0    [-n / ...] # run rss reader with or without updating cache
#        $0 -e [-n / ...] # also re-invoke editor with the set of chosen links

# run automatically UNLESS the file is sourced (sourcing would allow you to
# call the rss_skim function with other feeds and possibly different mangling/
# core cache update behaviour.

if [ "`caller`" = "0 NULL" ]; then
   f=~/.manga.chosen.new
   edit=
   if [ "$1" = "-e" ]; then
      rm $f; edit=1; shift
   fi

   rss_skim ~/.manga -P mangaaugment -p mangamangle ${1:+"$@"} http://feeds.feedburner.com/mangafox/latest_manga_chapters?format=xml http://www.onemanga.com/updates-feed.xml

   # immediately update the cache
   function M { perl -lpe 's/(?<=[^\x01])(, +(Vol\.|Ch\.)| +Chapter )/\x01$&/'; }
   grep -v '^#' ~/.manga.new >/dev/null && gr_NEWUPD -h -p M ~/.manga 2>&1 | fgrep -v '# gr_NEWUPD: ( echo "# gr_NEWUPD:'

   # allow user to re-invoke the editor (e.g. to view date stamps of the selection)
   if [ "$edit" != "" -a -s $f ]; then
      sleep 4; vi $f
   fi
fi

